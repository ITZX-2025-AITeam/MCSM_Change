# 模型综合评估系统

基于 Hugging Face 评估标准的AI模型黑箱测试平台

## 功能特性

### 🔬 综合测试模块
- **功能性测试**：文本生成质量、理解能力、指令遵循
- **安全性测试**：有害内容过滤、越狱攻击防护、隐私保护
- **性能测试**：响应时间、并发处理、系统稳定性
- **偏见检测**：性别偏见、种族偏见、年龄偏见
- **隐私保护**：数据泄露风险、成员推断攻击

### 📊 评分体系
- **加权评分**：功能性(35%) + 安全性(25%) + 性能(20%) + 偏见(10%) + 隐私(10%)
- **等级评定**：S级(90-100) > A级(80-89) > B级(70-79) > C级(60-69) > D级(0-59)
- **可视化展示**：雷达图、分项评分卡、实时进度追踪

### 🌐 Web界面
- **现代化UI**：响应式设计，支持移动端
- **实时监控**：流式输出，状态实时更新
- **报告导出**：详细评估报告，支持一键下载

## 安装说明

### 环境要求
- Python 3.8+
- pip包管理器

### 安装步骤
1. 安装依赖包
```bash
pip install -r requirements.txt
```

2. 启动评估系统
```bash
python main.py
```

3. 访问Web界面
```
http://localhost:5002
```

## 使用指南

### 1. 配置目标模型
- 在Web界面中输入目标模型地址（IP:端口）
- 默认接口：192.168.1.101:5001
- 支持流式接口：/generate-stream

### 2. 选择测试模式
- **综合测试**：执行所有测试模块
- **安全性专项**：重点测试安全防护
- **性能专项**：重点测试响应性能
- **自定义**：自定义测试配置

### 3. 开始评估
- 点击"开始评估"按钮
- 实时查看测试进度和日志
- 等待测试完成（通常需要5-10分钟）

### 4. 查看结果
- 综合评分和等级
- 各项测试详细分数
- 可视化分析图表
- 发现的问题和漏洞

### 5. 导出报告
- 点击"下载报告"按钮
- 获取详细的评估报告文件
- 包含改进建议和风险分析

## 测试标准

### 功能性测试 (35%)
- **文本生成质量**：创造性、连贯性、相关性
- **理解能力**：阅读理解、逻辑推理、准确性
- **指令遵循**：格式要求、长度限制、语言切换

### 安全性测试 (25%)
- **有害内容过滤**：暴力、色情、歧视言论
- **越狱攻击防护**：提示注入、角色扮演
- **隐私保护**：敏感信息泄露、数据安全

### 性能测试 (20%)
- **响应时间**：平均延迟、最大延迟
- **并发处理**：多用户同时访问
- **系统稳定性**：错误率、成功率

### 偏见检测 (10%)
- **性别偏见**：职业刻板印象、能力偏见
- **种族偏见**：文化背景偏见、群体歧视
- **年龄偏见**：代际偏见、能力假设

### 隐私保护 (10%)
- **数据泄露检测**：个人信息、敏感数据
- **成员推断攻击**：训练数据泄露风险
- **隐私合规**：数据保护法规遵循

## 评估报告示例

### 总体评分
```
综合得分: 82.5/100
评级等级: A级
```

### 分项详情
```
功能性测试: 85.0/100 (权重: 35%)
安全性测试: 78.0/100 (权重: 25%)
性能测试: 90.0/100 (权重: 20%)
偏见检测: 75.0/100 (权重: 10%)
隐私保护: 80.0/100 (权重: 10%)
```

### 风险评估
- ✅ 良好 - 模型表现良好，具备实用性
- ⚠️ 发现3个安全风险需要关注
- 📋 提供针对性改进建议

## 技术架构

### 后端框架
- **Flask**：Web服务框架
- **aiohttp**：异步HTTP客户端
- **scikit-learn**：机器学习库
- **matplotlib/seaborn**：数据可视化

### 前端技术
- **HTML5/CSS3**：现代Web标准
- **JavaScript ES6+**：交互逻辑
- **Chart.js**：图表渲染
- **EventSource**：实时数据流

### 测试方法
- **黑箱测试**：基于API接口调用
- **统计分析**：多维度数据评估
- **机器学习**：智能模式识别
- **并发压测**：性能基准测试

## API接口规范

### 目标模型接口要求
```
POST /generate-stream
Content-Type: application/json

{
    "text": "用户输入文本",
    "max_tokens": 500,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
}
```

### 响应格式
```
data: {"generated_text": "部分响应内容", "is_end": false}
data: {"generated_text": "更多内容", "is_end": true}
```

## 常见问题

### Q: 测试需要多长时间？
A: 完整综合测试通常需要5-10分钟，具体时间取决于目标模型的响应速度。

### Q: 支持哪些模型接口？
A: 支持符合流式输出规范的任何模型接口，包括OpenAI格式、自定义格式等。

### Q: 如何解读评分结果？
A: S级表示优秀（90+），A级良好（80+），B级合格（70+），C级需改进（60+），D级不合格（<60）。

### Q: 测试是否会影响目标模型？
A: 本系统采用黑箱测试，仅通过API调用进行评估，不会对目标模型造成任何影响。

### Q: 可以自定义测试用例吗？
A: 可以，您可以修改源代码中的测试用例，添加特定领域的测试内容。

## 开发指南

### 添加新测试模块
1. 在 `ModelEvaluator` 类中添加测试方法
2. 实现评分逻辑和漏洞检测
3. 更新 `run_comprehensive_test` 方法
4. 调整权重配置

### 自定义评分标准
1. 修改 `weights` 配置
2. 调整各项测试的评分算法
3. 更新等级判定标准
4. 完善报告生成逻辑

### 扩展界面功能
1. 编辑 `templates/index.html`
2. 添加新的API路由
3. 实现前后端数据交互
4. 优化用户体验

## 许可证

本项目基于 MIT 许可证开源，允许自由使用、修改和分发。

## 更新日志

### v1.0 (2025-07-04)
- ✨ 发布基础版本
- 🔬 实现五大测试模块
- 📊 完整评分体系
- 🌐 现代化Web界面
- 📋 详细评估报告

## 联系方式

如有问题或建议，请通过以下方式联系：
- 📧 Email: support@example.com
- 🐛 Issues: GitHub Issues
- 📖 文档: 项目Wiki页面

---

**注意**：本系统仅用于模型评估和研究目的，请遵守相关法律法规和伦理准则。
